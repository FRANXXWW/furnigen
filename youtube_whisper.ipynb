{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FRANXXWW/furnigen/blob/master/youtube_whisper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dt1Vq3yEwd1z"
      },
      "source": [
        "# YouTube Video Transcription with OpenAI's Whisper\n",
        "\n",
        "[![License](https://img.shields.io/github/license/kazuki-sf/youtube-whisper)](https://github.com/kazuki-sf/youtube-whisper)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/kazuki-sf/youtube-whisper/blob/main/youtube_whisper.ipynb)\n",
        "\n",
        "## How to Use the Notebook\n",
        "Feel free to `Copy to Drive` the notebook or run it directly.\n",
        "1. Enter the URL of the YouTube video or shorts you want to transcribe.\n",
        "2. Choose the whisper model you want to use.\n",
        "3. Run the code cell (Step 1-3) and wait for the transcription to complete.\n",
        "\n",
        "## Notes\n",
        "* `T4 GPU` or higher is recommended for running the notebook. You can change the runtime type by going to `Runtime` -> `Change runtime type` -> `Hardware accelerator` -> `GPU`.\n",
        "* Whenever you change the YouTube URL or Whisper Model, please run the `Step 1` and then run `Step 3` (You can skip `Step 2` if you already ran it before)\n",
        "* When you run `Step 3`, the website might ask you a permission to download multiple files.\n",
        "* This project is not affiliated with OpenAI. The code provided here is for educational purposes only.\n",
        "* Here's a list of whisper model and the relative speed of each model. For more information, please visit the official GitHub page: https://github.com/openai/whisper#available-models-and-languages\n",
        "---\n",
        "\n",
        "|  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "|:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "|  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "|  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "| small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "| medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "| large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "uF-0PMwmwd12"
      },
      "outputs": [],
      "source": [
        "# @title Step 1: Enter URL & Choose Whisper Model\n",
        "\n",
        "# @markdown Enter the URL of the YouTube video\n",
        "YouTube_URL = \"https://www.youtube.com/watch?v=NebZSnXt9hQ&t=186s\" #@param {type:\"string\"}\n",
        "\n",
        "# @markdown Choose the whisper model you want to use\n",
        "whisper_model = \"large-v3\" # @param [\"tiny\", \"base\", \"small\", \"medium\", \"large\", \"large-v2\", \"large-v3\"]\n",
        "\n",
        "# @markdown Save the transcription as text (.txt) file?\n",
        "text = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Save the transcription as an SRT (.srt) file?\n",
        "srt = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Translate the transcription?\n",
        "translate = True #@param {type:\"boolean\"}\n",
        "\n",
        "# @markdown Choose the target language for translation (e.g., 'es' for Spanish, 'fr' for French)\n",
        "target_language = \"es\" #@param {type:\"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWuOjqwqwd14",
        "outputId": "ea2be3d8-724f-4c85-ca48-c8264c552eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/57.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m891.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Install Dependencies (this may take about 2-3 min)\n",
        "\n",
        "!pip install -q pytube\n",
        "!pip install -q git+https://github.com/openai/whisper.git\n",
        "\n",
        "import os, re\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from pytube import YouTube\n",
        "import whisper\n",
        "from whisper.utils import get_writer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHPC53OAwd15",
        "outputId": "50dabde5-7cd9-4e62-bff9-11a3376105b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 2.88G/2.88G [00:50<00:00, 61.0MiB/s]\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Transcribe the video/audio data\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisper.load_model(whisper_model).to(device)\n",
        "\n",
        "# Util function to change name\n",
        "def to_snake_case(name):\n",
        "    return name.lower().replace(\" \", \"_\").replace(\":\", \"_\").replace(\"__\", \"_\")\n",
        "\n",
        "# Download the audio data from YouTube video\n",
        "def download_audio_from_youtube(url,  file_name = None, out_dir = \".\"):\n",
        "    print(f\"\\n==> Downloading audio...\")\n",
        "    yt = YouTube(url)\n",
        "    if file_name is None:\n",
        "        file_name = Path(out_dir, to_snake_case(yt.title)).with_suffix(\".mp4\")\n",
        "    yt = (yt.streams\n",
        "            .filter(only_audio = True, file_extension = \"mp4\")\n",
        "            .order_by(\"abr\")\n",
        "            .desc())\n",
        "    return yt.first().download(filename = file_name)\n",
        "\n",
        "# Import the Translator\n",
        "from googletrans import Translator\n",
        "\n",
        "# Transcribe the audio data with Whisper\n",
        "def transcribe_audio(model, file, text, srt, translate, target_language):\n",
        "    print(\"\\n=======================\")\n",
        "    print(f\"\\n🔗 YouTube URL: {YouTube_URL}\")\n",
        "    print(f\"\\n🤖 Whisper Model: {whisper_model}\")\n",
        "    print(\"\\n=======================\")\n",
        "\n",
        "    file_path = Path(file)\n",
        "    output_directory = file_path.parent\n",
        "\n",
        "    # Run Whisper to transcribe audio\n",
        "    print(f\"\\n==> Transcribing audio\")\n",
        "    result = model.transcribe(file, verbose = False)\n",
        "\n",
        "    transcribed_text = result[\"text\"]\n",
        "\n",
        "    if translate:\n",
        "        print(f\"\\n==> Translating to {target_language}\")\n",
        "        translator = Translator()\n",
        "\n",
        "        # Replace \"hacker\" and \"hackeo\" with placeholders before translation\n",
        "        transcribed_text_with_placeholders = transcribed_text.replace(\"hacker\", \"__HACKER__\").replace(\"hackeo\", \"__HACKEO__\")\n",
        "\n",
        "        try:\n",
        "            translated_text = translator.translate(transcribed_text_with_placeholders, dest=target_language).text\n",
        "            # Replace placeholders back with the original words\n",
        "            translated_text = translated_text.replace(\"__HACKER__\", \"hacker\").replace(\"__HACKEO__\", \"hackeo\")\n",
        "        except Exception as e:\n",
        "            print(f\"Translation failed: {e}\")\n",
        "            translated_text = \"Translation failed.\"\n",
        "\n",
        "    if text:\n",
        "        print(f\"\\n==> Creating .txt file\")\n",
        "        txt_path = file_path.with_suffix(\".txt\")\n",
        "        with open(txt_path, \"w\", encoding=\"utf-8\") as txt:\n",
        "            if translate:\n",
        "              txt.write(translated_text)\n",
        "            else:\n",
        "              txt.write(transcribed_text)\n",
        "\n",
        "    if srt:\n",
        "        print(f\"\\n==> Creating .srt file\")\n",
        "        srt_writer = get_writer(\"srt\", output_directory)\n",
        "        # You may need to modify the srt_writer or the result structure to handle translated text for SRT.\n",
        "        # For simplicity, this example will write the original transcription to SRT if translation is enabled.\n",
        "        # You would need a more advanced approach to get translated segments for SRT.\n",
        "        if translate:\n",
        "             print(\"SRT output with translation is not fully supported in this simple example.\")\n",
        "             # You would need to translate each segment in the 'result[\"segments\"]' if you want translated SRT\n",
        "        else:\n",
        "            srt_writer(result, str(file_path.stem))\n",
        "\n",
        "    # Download the transcribed files locally\n",
        "    from google.colab import files\n",
        "\n",
        "    colab_files = Path(\"/content\")\n",
        "    stem = file_path.stem\n",
        "\n",
        "    for colab_file in colab_files.glob(f\"{stem}*\"):\n",
        "        if colab_file.suffix in [\".txt\", \".srt\"]:\n",
        "            files.download(str(colab_file))\n",
        "\n",
        "    print(\"\\n✨ All Done!\")\n",
        "    print(\"=======================\")\n",
        "    return result\n",
        "\n",
        "# Download & Transcribe the audio data\n",
        "audio = download_audio_from_youtube(YouTube_URL)\n",
        "result = transcribe_audio(model, audio, text, srt, translate, target_language)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}